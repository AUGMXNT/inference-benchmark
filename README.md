We will use `shisa-7b` to test inference performance

Tests:
https://docs.google.com/spreadsheets/d/19YaxXkMJu7VweJihBMxQfMuz290Q3VxpqeG2DYCdRws/edit?usp=sharing

| Software        | Settings                              | Load Tokenizer                                                                 |   Load Model |   Inf JA 512/512 |   Tokens/Sec |   Inf EN 512/512 |   Tokens/Sec.1 | Avg Tok/s   |   Max Mem |   Speed X |     Memory | Notes                                                                                                                                          | Conversion         |
|:----------------|:--------------------------------------|:-------------------------------------------------------------------------------|-------------:|-----------------:|-------------:|-----------------:|---------------:|:------------|----------:|----------:|-----------:|:-----------------------------------------------------------------------------------------------------------------------------------------------|:-------------------|
| HF Transformers | Baseline (FP32)                       | 0.348583698272705                                                              |      8.01284 |        341.088   |      1.50108 |        352.478   |        1.45257 | 1.476825779 |     47677 |   1       |   1        |                                                                                                                                             |                 |
| HF Transformers | BF16                                  | 2.10556507110595                                                               |      4.53846 |        132.615   |      3.8608  |        131.349   |        3.898   | 3.879401805 |     46211 |   2.62777 |   0.969251 |                                                                                                                                             |                 |
| HF Transformers | BF16                                  | 0.662671327590942                                                              |      3.76796 |        131.968   |      3.87973 |        132.065   |        3.87688 | 3.878305424 |     46211 |   2.6268  |   0.969251 |                                                                                                                                             |                 |
|                 | torch.no_grad()                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | 0.347669839859008                                                              |      4.08075 |        131.631   |      3.88965 |        131.68    |        3.88822 | 3.888937545 |     45495 |   2.63401 |   0.954234 |                                                                                                                                             |                 |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | 0.339343786239624                                                              |      3.76552 |        118.696   |      4.31353 |        118.358   |        4.32587 | 4.319699801 |     47191 |   2.92585 |   0.989806 |                                                                                                                                             |                 |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | The model type mistral is not yet supported to be used with BetterTransformer. |           |               |           |               |             | #DIV/0!     |        |        |         |                                                                                                                                             |                 |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | Pptimum BetterTransformer             |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | 0.339448928833007                                                              |      4.09873 |        119.351   |      4.28986 |        118.895   |        4.30633 | 4.298095414 |     46851 |   2.91124 |   0.982675 |                                                                                                                                             |                 |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | 0.34398341178894                                                               |      5.7032  |        246.55    |      2.07666 |        249.162   |        2.05489 | 2.065772873 |     42623 |   1.39905 |   0.893995 | UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization                                              |                 |
|                 | load_in_8bit=True                     |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | BF16                                  | 0.350092649459838                                                              |      5.42949 |        247.355   |      2.0699  |        248.626   |        2.05932 | 2.064611797 |     45127 |   1.39832 |   0.946515 | UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization                                              |                 |
|                 | load_in_8bit=True                     |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | bnb_8bit_compute_dtype=torch.bfloat16 |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | FP16                                  | 0.342193365097045                                                              |      5.5427  |        241.813   |      2.11733 |        239.586   |        2.13702 | 2.127178175 |     45511 |   1.44087 |   0.954569 | https://github.com/TimDettmers/bitsandbytes/issues/490                                                                                         |                 |
|                 | load_in_8bit=True                     |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | bnb_4bit_compute_dtype=torch.float16  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | FP16                                  | 0.353858470916748                                                              |      5.98889 |        241.677   |      2.11853 |        243.907   |        2.09916 | 2.108842953 |     45509 |   1.42823 |   0.954527 |                                                                                                                                             |                 |
|                 | load_in_8bit=True                     |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | bnb_4bit_compute_dtype=torch.float16  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.compile()                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| HF Transformers | FP16                                  | 0.344361066818237                                                              |      6.02854 |        145.955   |      3.50793 |        146.13    |        3.50374 | 3.505835079 |     44101 |   2.37452 |   0.924995 |                                                                                                                                             |                 |
|                 | load_in_4bit=True                     |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | bnb_4bit_compute_dtype=torch.float16  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.compile()                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | torch.inference_mode()                |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | use_flash_attention_2=True            |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | SDPA flash attention                  |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| vLLM            | tensor_parallel_size=1                | 0.337995529174804                                                              |      9.52003 |          9.27908 |     55.1779  |          9.24633 |       55.3733  | 55.27561834 |     19958 |  37.4398  |   0.418609 | vLLM is fast even for batch=1 but you need to batch by SamplerSettings and also you can't batch w/ multiple seeds                              |                 |
| vLLM            | tensor_parallel_size=2                | 0.346305847167968                                                              |     17.6519  |          7.50553 |     68.2163  |          7.48511 |       68.4025  | 68.30940794 |     47843 |  46.2677  |   1.00348  | A copy on each GPU                                                                                                                             |                 |
| vLLM            | tensor_parallel_size=2                | 0.346305847167968                                                              |     20.0429  |          5.91868 |     86.5058  |          5.87759 |       87.1106  | 86.80818315 |     47175 |  58.7995  |   0.989471 |                                                                                                                                             |                 |
|                 | quantization='awq'                    |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| vLLM            | tensor_parallel_size=2                |                                                                             |           |               |           |               |             |          |        |        |         | NotImplementedError: Pipeline parallelism is not supported yet.                                                                                |                 |
|                 | pipeline_parallel_size=2              |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
|                 | quantization='awq'                    |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                |                    |
| cTranslate2     |                                    | 0.345565795898437                                                              |      3.7232  |          9.21155 |     55.5824  |          9.12135 |       56.132   | 55.85720109 |     16996 |  37.8357  |   0.356482 | requires model conversion: https://opennmt.net/CTranslate2/conversion.html                                                                     | real    0m59.672s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    1m8.717s   |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            | Missing some of the usual generation parameters, 4090 only                                                                                     | sys     0m20.583s  |
| llama.cpp       | fp16                                  |                                                                             |      2.57498 |         12.376   |     41.3703  |         12.6455  |       40.4887  | 40.92950342 |     17987 |  27.7171  |   0.377268 | convert_shisa.py                                                                                                                               | real    0m51.034s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            | 4090+3090                                                                                                                                      | user    0m38.386s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     0m25.292s  |
| llama.cpp       | fp16                                  |                                                                             |      2.4559  |          9.34834 |     54.7691  |          9.40606 |       54.433   | 54.6010376  |     15873 |  36.98    |   0.332928 | 4090 only                                                                                                                                      |                 |
| llama.cpp       | q8                                    |                                                                             |      1.55778 |         10.2952  |     49.7317  |         10.6275  |       48.1767  | 48.95417263 |     11541 |  33.1485  |   0.242066 | 4090+3090                                                                                                                                      | real    0m9.897s   |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    1m32.065s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     0m15.903s  |
| llama.cpp       | q8                                    |                                                                             |      1.48527 |          5.78249 |     88.5432  |          5.8751  |       87.1475  | 87.84531183 |      9919 |  59.4908  |   0.208046 | 4090 only                                                                                                                                      |                 |
| llama.cpp       | q4_k_m                                |                                                                             |      1.07488 |          9.48225 |     53.9956  |          9.81459 |       52.1672  | 53.08142803 |      8271 |  35.9424  |   0.17348  | 4.63 BPW                                                                                                                                       | real    0m28.234s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            | 4090+3090                                                                                                                                      | user    12m2.182s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     0m12.564s  |
| llama.cpp       | q4_k_m                                |                                                                             |      1.48527 |          3.99931 |    128.022   |          4.0858  |      125.312   | 126.6670701 |      6701 |  85.7779  |   0.14055  | 4090 only                                                                                                                                      |                 |
| ExLLamaV2       | EXLV2 8 BPW                           |                                                                             |           |          5.56327 |     92.0322  |          5.45558 |       93.8489  | 92.94055324 |     13688 |  62.9597  |   0.287099 | 4090 only                                                                                                                                      | real    22m35.572s |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    67m16.503s |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     11m45.998s |
| ExLLamaV2       | EXLV2 4.63 BPW                        |                                                                             |           |          3.87498 |    132.13    |          3.7461  |      136.676   | 134.4026303 |     10856 |  91.0576  |   0.227699 | 4090 only                                                                                                                                      | real    6m13.610s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    36m10.737s |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     6m23.130s  |
| ExLLamaV2       | GPTQ Q4 GS128 actorder                |                                                                             |           |          3.91668 |    130.723   |          3.86658 |      132.417   | 131.569843  |     10938 |  89.1231  |   0.229419 | 4090 only                                                                                                                                      | real    6m13.610s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    36m10.737s |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     6m23.130s  |
| MLC LLM         | q0f16                                 |                                                                             |           |               |           |               |             |          |        |        |         |                                                                                                                                             |                 |
| MLC LLM         | q8f16_1                               |                                                                             |           |               |           |               |             |          |        |        |         |                                                                                                                                             |                 |
| MLC LLM         | q4f16_1                               |                                                                             |           |               |           |               |             |          |        |        |         | mlc_chat_cli: symbol lookup error: ... mlc-llm/dist/shisa-7b-v1-q4f16_1/shisa-7b-v1-q4f16_1-cuda.so: undefined symbol: __cudaRegisterFatBinary | real    1m23.850s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | user    1m37.217s  |
|                 |                                       |                                                                                |              |                  |              |                  |                |             |           |           |            |                                                                                                                                                | sys     0m41.900s  |
| MLC LLM         | autogptq_llama_q4f16_1                |                                                                             |           |               |           |               |             |          |        |        |         |                                                                                                                                             |                 |
| gpt-fast        |                                    |                                                                             |           |               |           |               |             |          |        |        |         |                                                                                                                                             |                 |

